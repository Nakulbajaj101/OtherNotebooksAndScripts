{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import create_dataset, create_bq_table\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from analysisConfig import cluster_analysis as analysis\n",
    "from analysisConfig import connection_string, integration_dataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import gmaps\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "import statsmodels.api as sm\n",
    "from scipy import spatial\n",
    "from analysisConfig import places_api as api_key\n",
    "gmaps.configure(api_key=api_key)\n",
    "from io import StringIO\n",
    "import psycopg2\n",
    "import os\n",
    "import gc\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_locations = \"/Users/bajajn2/ShortcutScripts/newScripts/\"+analysis.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = analysis.project\n",
    "dataset = analysis.dataset\n",
    "latest_feature_dataset = analysis.feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis.company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(project = project, dataset_id=dataset, confirm_dataset='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bq_table(project=project, dataset = dataset, file_location=query_locations, table = 'company_cust', feature_table=latest_feature_dataset, integration_table = integration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bq_table(project=project, dataset = dataset, file_location=query_locations, table = 'company_cust_filtered', feature_table=latest_feature_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bq_table(project=project, dataset = dataset, file_location=query_locations, table = 'custpostcode_service_spending', feature_table=latest_feature_dataset, integration_table=integration_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_gbq(\"\"\"select * from `cluster_analysis.company_cust_filtered` where cmpnyid = {}\"\"\".format(analysis.company), project, dialect = 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_subplots_4(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "\n",
    "    plt.figure(figsize = (20,20))\n",
    "\n",
    "    ax = plt.subplot(4, 1, 1); sns.distplot(df['Recency'])\n",
    "    ax.set_xlabel('Recency')\n",
    "    # Plot frequency distribution\n",
    "    ax1 = plt.subplot(4, 1, 2); sns.distplot(df['Frequency'])\n",
    "    ax1.set_xlabel('Frequency')\n",
    "\n",
    "    # Plot monetary value distribution\n",
    "    ax2 = plt.subplot(4, 1, 3); sns.distplot(df['Monetary_value'])\n",
    "    ax2.set_xlabel(\"Monetary Value\")\n",
    "\n",
    "    #Plot Tenure distriburtion\n",
    "    ax3 = plt.subplot(4, 1, 4); sns.distplot(df['Tenure'])\n",
    "    ax3.set_xlabel(\"Tenure\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_stats(data, normalized_data, clusters = 2):\n",
    "    df = data.copy()\n",
    "    df_normalized = normalized_data\n",
    "    kmeans = KMeans(n_clusters=clusters, random_state=1, n_jobs=-1) \n",
    "\n",
    "    # Fit k-means clustering on the normalized data set\n",
    "    kmeans.fit(df_normalized)\n",
    "\n",
    "    # Extract cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "    df_rfm = df.assign(Cluster=cluster_labels)\n",
    "    df_normalized = df_normalized.assign(Cluster = cluster_labels)\n",
    "\n",
    "    # Group the data by cluster\n",
    "    grouped = df_rfm.groupby(['Cluster'])\n",
    "\n",
    "    # Calculate average RFM values and segment sizes per cluster value\n",
    "    df_grouped = grouped.agg({\n",
    "        'Recency': 'mean',\n",
    "        'Frequency': 'mean',\n",
    "        'Tenure': 'mean',\n",
    "        'Monetary_value': ['mean', 'count']\n",
    "      }).round(1)\n",
    "    return df_grouped, df_normalized, df_rfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[['Recency','Frequency','Monetary_value','Tenure']]\n",
    "df_log = np.log(df_subset)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_log)\n",
    "\n",
    "# Scale and center the data\n",
    "df_normalized = scaler.transform(df_log)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_normalized = pd.DataFrame(data=df_normalized, index=df_subset.index, columns=df_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_subplots_4(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_subplots_4(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = {}\n",
    "silhouette_average =  []\n",
    "for k in range(2,8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1, n_jobs=-1)\n",
    "    kmeans.fit(df_normalized)\n",
    "    #cluster_labels = kmeans.fit_predict(df_normalized)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    #silhouette_average.append(silhouette_score(df_normalized, cluster_labels,sample_size=20000, random_state=1))\n",
    "    #print(\"For n_clusters =\", k,\n",
    "    #      \"The average silhouette_score is :\", silhouette_average[k-2])\n",
    "    sse[k] = kmeans.inertia_\n",
    "    \n",
    "Optimal_clusters_inertia = np.argmax([k for k in np.abs([j for j in np.diff([i for i in sse.values()])] + [0-[i for i in sse.values()][-1]])/np.array([i for i in sse.values()]) if k < 1]) + 3\n",
    "#Optimal_clusters_silhoutte = np.argmax(silhouette_average) + 2\n",
    "print(\"Optimal number of clusters chosen by k means inertia is {} and by silhoutte distance is not known\".format(Optimal_clusters_inertia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the plot title \"The Elbow Method\"\n",
    "plt.title('The Elbow Method')\n",
    "\n",
    "# Add X-axis label \"k\"\n",
    "plt.xlabel('k')\n",
    "\n",
    "# Add Y-axis label \"SSE\"\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each key in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = cluster_stats(df, df_normalized, clusters=Optimal_clusters_inertia)\n",
    "df_normalized = df_grouped[1]\n",
    "df_melt = pd.melt(df_normalized.reset_index(), \n",
    "                    id_vars=['Cluster'],\n",
    "                    value_vars=['Recency', 'Frequency', 'Monetary_value', 'Tenure'], \n",
    "                    var_name='Attribute', \n",
    "                    value_name='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Snake plot of standardized variables')\n",
    "sns.lineplot(x=\"Attribute\", y=\"Value\", hue='Cluster', data=df_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Recency\",\"Frequency\",\"Monetary_value\", 'Tenure']\n",
    "cluster_average = df_grouped[2][columns + ['Cluster']].groupby(['Cluster']).mean()\n",
    "population_average = df_grouped[2][columns].mean()\n",
    "relative_importance = cluster_average/population_average - 1\n",
    "Important_cluster = np.argmax(np.sum(relative_importance[['Frequency','Monetary_value']],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_importance = cluster_average/population_average - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9.7, 4))\n",
    "plt.title('cluster average')\n",
    "sns.heatmap(data=cluster_average, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9.7, 4))\n",
    "plt.title('Relative importance of attributes')\n",
    "sns.heatmap(data=relative_importance, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped[2]['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Important_cluster = df_grouped[2][df_grouped[2].Cluster == Important_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_table_percent(data, agg, index, columns, values, percent = True):\n",
    "    df = data.copy()\n",
    "    df_pivot = df.pivot_table(index = index, columns = columns, values = values, aggfunc = agg).reset_index()\n",
    "    if percent == True:\n",
    "        df_pivot[values] = [i/df_pivot[values].sum() for i in df_pivot[values]]\n",
    "    return df_pivot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_visualisation(data, datafrom, agg, columns = None, values = None, index = None, percent = True ):\n",
    "    df = data.copy()\n",
    "    df1 = datafrom.copy()\n",
    "    df = pivot_table_percent(df, agg, index, columns, values, percent)\n",
    "    df1 = pivot_table_percent(df1, agg, index, columns, values, percent)\n",
    "    df['ratio'] = (df[values]/df1[values] - 1)*100\n",
    "    plt.figure(figsize=(9.7, 4))\n",
    "    \n",
    "    ax = sns.barplot(x = df[index], y = df[\"ratio\"])\n",
    "    ax.set_title(\"Relative change in {} of {}\".format(agg,index))\n",
    "    ax.set(ylabel = \"ratio %\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'count', values = 'custid', index = \"agebin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'count', values = 'custid', index = \"cust_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'count', values = 'custid', index = \"cust_gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'mean', values = 'txnval_total', index = \"agebin\", percent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'mean', values = 'txnval_total', index = \"cust_state\", percent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'mean', values = 'txnval_total', index = \"cust_gender\", percent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'mean', values = 'txncnt_cnt', index = \"agebin\", percent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'mean', values = 'txncnt_cnt', index = \"cust_gender\", percent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_visualisation(df_Important_cluster, df_grouped[2],agg = 'mean', values = 'txncnt_cnt', index = \"cust_state\", percent = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_grouped[2].drop(['Cluster'],1)\n",
    "df_main['Cluster'] = 'All'\n",
    "df_final = pd.concat([df_grouped[2],df_main],axis = 0, ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_agebin_mean = df_final.groupby(['Cluster','agebin']).agg({'Recency':'mean','Frequency':'mean','Monetary_value':'mean','Tenure':'mean'}).reset_index()\n",
    "df_final_agebin_mean['ClusterAgebin'] = [(str(i) + \" : \" + j) for i,j in zip(df_final_agebin_mean.Cluster,df_final_agebin_mean.agebin)]\n",
    "df_final_agebin_mean = df_final_agebin_mean.drop(['Cluster','agebin'],1)\n",
    "df_final_agebin_mean_subset = df_final_agebin_mean[[\"Recency\",\"Frequency\",\"Monetary_value\",\"Tenure\"]]\n",
    "df_final_agebin_mean_log = np.log(df_final_agebin_mean_subset)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_final_agebin_mean_log)\n",
    "\n",
    "# Scale and center the data\n",
    "df_final_agebin_mean_normalized = scaler.transform(df_final_agebin_mean_log)\n",
    "\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_final_agebin_mean_normalized = pd.DataFrame(data=df_final_agebin_mean_normalized, index=df_final_agebin_mean_subset.index, columns=df_final_agebin_mean_subset.columns)\n",
    "df_final_agebin_mean_normalized = df_final_agebin_mean_normalized.assign(Cluster = df_final_agebin_mean['ClusterAgebin'])\n",
    "df_final_agebin_melt = pd.melt(df_final_agebin_mean_normalized.reset_index(), \n",
    "                    id_vars=['Cluster'],\n",
    "                    value_vars=['Recency', 'Frequency', 'Monetary_value', 'Tenure'], \n",
    "                    var_name='Attribute', \n",
    "                    value_name='Value')\n",
    "plt.figure(figsize=(20,15))\n",
    "ax = sns.lineplot(x=\"Attribute\", y=\"Value\", hue='Cluster', data=df_final_agebin_melt)\n",
    "ax.set_title('Snake plot of standardized variables')\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_cust_gender_mean = df_final.groupby(['Cluster','cust_gender']).agg({'Recency':'mean','Frequency':'mean','Monetary_value':'mean','Tenure':'mean'}).reset_index()\n",
    "df_final_cust_gender_mean['Clustercust_gender'] = [(str(i) + \" : \" + j) for i,j in zip(df_final_cust_gender_mean.Cluster,df_final_cust_gender_mean.cust_gender)]\n",
    "df_final_cust_gender_mean = df_final_cust_gender_mean.drop(['Cluster','cust_gender'],1)\n",
    "df_final_cust_gender_mean_subset = df_final_cust_gender_mean[[\"Recency\",\"Frequency\",\"Monetary_value\",\"Tenure\"]]\n",
    "df_final_cust_gender_mean_log = np.log(df_final_cust_gender_mean_subset)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_final_cust_gender_mean_log)\n",
    "\n",
    "# Scale and center the data\n",
    "df_final_cust_gender_mean_normalized = scaler.transform(df_final_cust_gender_mean_log)\n",
    "\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_final_cust_gender_mean_normalized = pd.DataFrame(data=df_final_cust_gender_mean_normalized, index=df_final_cust_gender_mean_subset.index, columns=df_final_cust_gender_mean_subset.columns)\n",
    "df_final_cust_gender_mean_normalized = df_final_cust_gender_mean_normalized.assign(Cluster = df_final_cust_gender_mean['Clustercust_gender'])\n",
    "df_final_cust_gender_melt = pd.melt(df_final_cust_gender_mean_normalized.reset_index(), \n",
    "                    id_vars=['Cluster'],\n",
    "                    value_vars=['Recency', 'Frequency', 'Monetary_value', 'Tenure'], \n",
    "                    var_name='Attribute', \n",
    "                    value_name='Value')\n",
    "plt.figure(figsize=(20,15))\n",
    "ax = sns.lineplot(x=\"Attribute\", y=\"Value\", hue='Cluster', data=df_final_cust_gender_melt)\n",
    "ax.set_title('Snake plot of standardized variables')\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_cust_state_mean = df_final.groupby(['Cluster','cust_state']).agg({'Recency':'mean','Frequency':'mean','Monetary_value':'mean','Tenure':'mean'}).reset_index()\n",
    "df_final_cust_state_mean['Clustercust_state'] = [(str(i) + \" : \" + j) for i,j in zip(df_final_cust_state_mean.Cluster,df_final_cust_state_mean.cust_state)]\n",
    "df_final_cust_state_mean = df_final_cust_state_mean.drop(['Cluster','cust_state'],1)\n",
    "df_final_cust_state_mean_subset = df_final_cust_state_mean[[\"Recency\",\"Frequency\",\"Monetary_value\",\"Tenure\"]]\n",
    "df_final_cust_state_mean_log = np.log(df_final_cust_state_mean_subset)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_final_cust_state_mean_log)\n",
    "\n",
    "# Scale and center the data\n",
    "df_final_cust_state_mean_normalized = scaler.transform(df_final_cust_state_mean_log)\n",
    "\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_final_cust_state_mean_normalized = pd.DataFrame(data=df_final_cust_state_mean_normalized, index=df_final_cust_state_mean_subset.index, columns=df_final_cust_state_mean_subset.columns)\n",
    "df_final_cust_state_mean_normalized = df_final_cust_state_mean_normalized.assign(Cluster = df_final_cust_state_mean['Clustercust_state'])\n",
    "df_final_cust_state_melt = pd.melt(df_final_cust_state_mean_normalized.reset_index(), \n",
    "                    id_vars=['Cluster'],\n",
    "                    value_vars=['Recency', 'Frequency', 'Monetary_value', 'Tenure'], \n",
    "                    var_name='Attribute', \n",
    "                    value_name='Value')\n",
    "plt.figure(figsize=(20,15))\n",
    "ax = sns.lineplot(x=\"Attribute\", y=\"Value\", hue='Cluster', data=df_final_cust_state_melt)\n",
    "ax.set_title('Snake plot of standardized variables')\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.05))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(data, datafrom, agg, columns = None, values = None, index = None, percent = True, top = 100, customers = 50 ):\n",
    "    df = data.copy()\n",
    "    df1 = datafrom.copy()\n",
    "    df = pivot_table_percent(df, agg, index, columns, values, percent)\n",
    "    df1 = pivot_table_percent(df1, agg, index, columns, values, percent)\n",
    "    df = pd.merge(df,df1, on = index)\n",
    "    df['ratio'] = (df[values + '_x']/df[values + '_y'])*100\n",
    "    if agg == 'count':\n",
    "        df = df[df[values + '_y'] >= customers]    \n",
    "    df = df.sort_values('ratio', ascending=False).reset_index().drop(['index'],1)\n",
    "    df[index] = df[index].astype('int64')\n",
    "    print(df.shape)\n",
    "    df_top = df[0:int(df.shape[0]/5)]\n",
    "    if df_top.shape[0] > top:\n",
    "        df_top = df_top[0:top]\n",
    "    df_bottom = df[int((df.shape[0]/5)*4):]\n",
    "    df_bottom = df_bottom.reset_index().drop(['index'],1)\n",
    "    if df_bottom.shape[0] > top:\n",
    "        df_bottom = df_bottom.tail(top)\n",
    "        df_bottom = df_bottom.reset_index().drop(['index'],1)\n",
    "    df_remaining = df[(df[index].isin([i for i in df_bottom[index]]) == False) & (df[index].isin([i for i in df_top[index]]) == False) ]\n",
    "    df_remaining = df_remaining.reset_index().drop(['index'],1)\n",
    "    return df_top, df_bottom, df_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode_hotspots, df_postcode_bottom, df_remaining = change(df_Important_cluster, df_grouped[2], 'count', values = 'custid',index = 'cust_postcode', percent=False, customers = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining[df_remaining.cust_postcode == 2750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode_bottom[df_postcode_bottom.cust_postcode == 2750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped[2][df_grouped[2].cust_postcode == \"2750\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode_hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opportunity_postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "select postcode, lat, lon\n",
    "          from\n",
    "          (\n",
    "          select postcode as postcode1, sum(total_persons_males + total_persons_females) as total_count\n",
    "          from\n",
    "         `dev_external_data.postcode_features_2016`\n",
    "          group by postcode)\n",
    "\n",
    "inner join \n",
    "\n",
    "`dev_external_data.postcode` on\n",
    "postcode1 = safe_cast(postcode as int64)\n",
    "where country_code = \"au\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode = pd.read_gbq(query, project_id=project, dialect=\"standard\")\n",
    "df_postcode.postcode = df_postcode.postcode.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode_hotspots = pd.merge(df_postcode_hotspots, df_postcode, left_on='cust_postcode', right_on='postcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postcode_hotspots.to_csv(\"C:/Users/bajaj/Desktop/hotspots.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map centered on London\n",
    "fig = gmaps.figure()\n",
    "heatmap_layer = gmaps.heatmap_layer(df_postcode_hotspots[['lat','lon']], weights=df_postcode_hotspots.custid_x, max_intensity=100,point_radius=5, opacity=1)\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from `dev_external_data.postcode_features_2016`\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_features = pd.read_gbq(query, project, dialect = \"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select * from `cluster_analysis.custpostcode_service_spending`\"\"\"\n",
    "postcode_spending_features = pd.read_gbq(query, project, dialect = \"standard\")\n",
    "\n",
    "\n",
    "postcode_spending_features['service_code'] = ['service_code_'+ str(i) for i in postcode_spending_features['service_code']]\n",
    "\n",
    "\n",
    "postcode_service_code_txnval_total = pd.pivot_table(postcode_spending_features, columns='service_code', index='cust_postcode', values='txnval_total',fill_value=0).reset_index()\n",
    "postcode_service_code_txnval_total = postcode_service_code_txnval_total.rename(columns={i:i+'_txnval_total' for i in postcode_service_code_txnval_total if 'service_code' in i})\n",
    "postcode_service_code_txncnt_cnt = pd.pivot_table(postcode_spending_features, columns='service_code', index='cust_postcode', values='txncnt_cnt',fill_value=0).reset_index()\n",
    "postcode_service_code_txncnt_cnt = postcode_service_code_txncnt_cnt.rename(columns={i:i+'_txncnt_cnt' for i in postcode_service_code_txncnt_cnt if 'service_code' in i})\n",
    "postcode_service_code_custcnt_cnt = pd.pivot_table(postcode_spending_features, columns='service_code', index='cust_postcode', values='custcnt_cnt',fill_value=0).reset_index()\n",
    "postcode_service_code_custcnt_cnt = postcode_service_code_custcnt_cnt.rename(columns={i:i+'_custcnt_cnt' for i in postcode_service_code_custcnt_cnt if 'service_code' in i})\n",
    "\n",
    "\n",
    "\n",
    "postcode_service_code_features = pd.merge(postcode_service_code_custcnt_cnt, postcode_service_code_txnval_total, on = 'cust_postcode', how = 'inner')\n",
    "postcode_service_code_features = pd.merge(postcode_service_code_features, postcode_service_code_txncnt_cnt, on = 'cust_postcode', how='inner')\n",
    "scaler = MinMaxScaler()\n",
    "postcode_X = postcode_service_code_features[[i for i in postcode_service_code_features if 'service_code' in i]]\n",
    "scaler.fit(postcode_X)\n",
    "postcode_normalised = scaler.transform(postcode_X)\n",
    "postcode_service_code_features_normalised = pd.DataFrame(data=postcode_normalised, index = postcode_X.index, columns = postcode_X.columns)\n",
    "postcode_service_code_features_normalised = postcode_service_code_features_normalised.assign(cust_postcode = postcode_service_code_features.cust_postcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postcode_service_code_features.to_gbq('cluster_analysis.postcode_service_code_features', 'anz-insto-data-analytics_dev',if_exists='replace' )\n",
    "#postcode_service_code_features_normalised.to_gbq('cluster_analysis.postcode_service_code_features_normalised', 'anz-insto-data-analytics_dev',if_exists='replace' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_pct_features = [i for i in postcode_features if 'pct' in i]\n",
    "postcode_dwelling_features = [i for i in postcode_features if \"dwelling\" in i]\n",
    "postcode_features = postcode_features[postcode_features.total_persons_males + postcode_features.total_persons_females >= 1000].reset_index().drop([\"index\"],1)\n",
    "columns = [\"postcode\"] + postcode_pct_features + postcode_dwelling_features\n",
    "postcode_features = postcode_features[columns]\n",
    "postcode_features_subset = postcode_features[postcode_dwelling_features]\n",
    "scaleminmax = MinMaxScaler()\n",
    "scaleminmax.fit(postcode_features_subset)\n",
    "postcode_features_scaled = scaleminmax.transform(postcode_features_subset)\n",
    "postcode_features_scaled = pd.DataFrame(postcode_features_scaled, index = postcode_features_subset.index,columns = postcode_features_subset.columns )\n",
    "postcode_features = postcode_features[[\"postcode\"] + postcode_pct_features]\n",
    "postcode_features = postcode_features[[ i for i in postcode_features if postcode_features[i].dtype != \"O\"]]\n",
    "for i in postcode_features:\n",
    "    if \"pct\" in i:\n",
    "        #print(i)\n",
    "        postcode_features[i] = list(map(lambda x: x/100 if x > 1 else x, postcode_features[i]))\n",
    "postcode_features = pd.concat([postcode_features, postcode_features_scaled], axis = 1)\n",
    "postcode_features[\"hotspot\"] = list(map(lambda x : 1 if x in [i for i in df_postcode_hotspots.cust_postcode] else 0,[j for j in postcode_features.postcode])) \n",
    "postcode_service_code_features_normalised = postcode_service_code_features_normalised.rename(columns = {'cust_postcode':'postcode'})\n",
    "postcode_service_code_features_normalised.postcode = postcode_service_code_features_normalised.postcode.astype(int)\n",
    "postcode_features = pd.merge(postcode_service_code_features_normalised,postcode_features, how = 'inner', on = 'postcode')\n",
    "TrainingColumns = [i for i in postcode_features if i not in [\"postcode\",\"hotspot\"]]\n",
    "X = postcode_features[TrainingColumns]\n",
    "y = postcode_features[\"hotspot\"]\n",
    "postcode_features_Test = postcode_features[postcode_features.hotspot == 0]\n",
    "postcode_features_Test = postcode_features_Test.reset_index().drop(['index'],1)\n",
    "postcode_features_Test_Predict = postcode_features_Test[TrainingColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_probability(TrainX,Trainy, TestX, TestDataframe, model = None, top = 100):\n",
    "    if model == \"rfc\":\n",
    "        m = RandomForestClassifier(random_state=1,n_jobs=-1, class_weight='balanced')\n",
    "        m.fit(TrainX,Trainy)\n",
    "        TestDataframe[\"Probability\"] = [i[1] for i in m.predict_proba(TestX) ]\n",
    "        TestDataframe = TestDataframe.sort_values('Probability', ascending = False).reset_index().drop([\"index\"],1)[0:top]\n",
    "    elif model == \"lr\":\n",
    "        m = LogisticRegression(class_weight='balanced', random_state=1)\n",
    "        m.fit(TrainX,Trainy)\n",
    "        logit_model=sm.Logit(Trainy,TrainX)\n",
    "        result=logit_model.fit(method='bfgs')\n",
    "        print(result.summary2())\n",
    "\n",
    "        TestDataframe[\"Probability\"] = [i[1] for i in m.predict_proba(TestX) ]\n",
    "        feature_importance = abs(m.coef_[0])\n",
    "        feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "        #feature_importance\n",
    "        \n",
    "        TestDataframe = TestDataframe[TestDataframe.Probability >= 0.8]  \n",
    "        TestDataframe = TestDataframe.sort_values('Probability', ascending = False).reset_index().drop([\"index\"],1)  \n",
    "        \n",
    "        print([(i ,ind) for ind, i in enumerate(feature_importance) if i > 50])\n",
    "        sorted_idx = np.argsort(feature_importance)\n",
    "        #print(sorted_idx)\n",
    "        pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "        featfig = plt.figure(figsize = (20,10))\n",
    "        featax = featfig.add_subplot(1, 1, 1)\n",
    "        featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "        featax.set_yticks(pos)\n",
    "        featax.set_yticklabels(np.array(X.columns)[sorted_idx], fontsize=8)\n",
    "        featax.set_xlabel('Relative Feature Importance')\n",
    "\n",
    "        plt.tight_layout()   \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    TestDataframe.Probability = [format(i,'f') for i in TestDataframe.Probability]\n",
    "    return TestDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_probability(X,y, postcode_features_Test_Predict, postcode_features_Test, model = 'rfc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_probability(X,y, postcode_features_Test_Predict, postcode_features_Test, model = 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(l1, l2):\n",
    "    result = 1 - spatial.distance.cosine(l1, l2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = {}\n",
    "l2 = {}\n",
    "l3 = {}\n",
    "for i in postcode_features['postcode']:\n",
    "    if i in [j for j in df_postcode_hotspots.cust_postcode]:\n",
    "        l1[i] = postcode_features[postcode_features['postcode'] == i][[j for j in X.columns]].values.tolist()   \n",
    "    else:\n",
    "        if i in [j for j in df_postcode_bottom.cust_postcode]:\n",
    "            l2[i] = postcode_features[postcode_features['postcode'] == i][[j for j in X.columns ]].values.tolist()   \n",
    "        elif i in [j for j in df_remaining.cust_postcode]:\n",
    "            l3[i] = postcode_features[postcode_features['postcode'] == i][[j for j in X.columns ]].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode1 = {}\n",
    "for k in l3:\n",
    "    scores1 = []\n",
    "    scores2 = []\n",
    "    postcode2 = []\n",
    "    postcode3 = []\n",
    "    for j in l2:\n",
    "        scores1.append(cosine_similarity(l3[k],l2[j]))\n",
    "        postcode2.append(j)\n",
    "    for i in l1:\n",
    "        scores2.append(cosine_similarity(l3[k],l1[i]))\n",
    "        postcode3.append(i)\n",
    "    if np.max(scores1) < np.max(scores2):\n",
    "        postcode1[k] = postcode3[np.argmax(scores2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opportunity_postcodes = pd.DataFrame.from_dict(postcode1, 'index').reset_index().rename(columns = {'index':'postcode',0:'SimilarPostcode'})\n",
    "df_postcode.postcode = df_postcode.postcode.astype('int64')\n",
    "opportunity_postcode_hotspots = pd.merge(opportunity_postcodes, df_postcode, on = 'postcode')\n",
    "opportunity_postcode_hotspots = opportunity_postcode_hotspots.reset_index().drop(['index'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = gmaps.figure(center=(-37.8102, 144.96), zoom_level=5)\n",
    "marker_layer = gmaps.marker_layer(opportunity_postcode_hotspots[['lat','lon']], info_box_content= ['postcode:' + str(i) for i in opportunity_postcode_hotspots.postcode], hover_text=['Similar to postcode: ' + str(i) for i in opportunity_postcode_hotspots['SimilarPostcode']])\n",
    "marker_layer_2 = gmaps.symbol_layer(df_postcode_hotspots[['lat','lon']], info_box_content=['postcode:' + str(i) for i in df_postcode_hotspots.postcode])\n",
    "fig.add_layer(marker_layer)\n",
    "fig.add_layer(marker_layer_2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5, weights='distance')  \n",
    "top_postcode = postcode_features[postcode_features.postcode.isin([i for i in l1])].reset_index().drop(['index','hotspot'],1).assign(Label = 1)\n",
    "bottom_postcode = postcode_features[postcode_features.postcode.isin([i for i in l2])].reset_index().drop(['index','hotspot'],1).assign(Label = -1)\n",
    "remaining_postcode = postcode_features[postcode_features.postcode.isin([i for i in l3])].reset_index().drop(['index','hotspot'],1)\n",
    "postcode_ranking = pd.concat([top_postcode, bottom_postcode], axis=0, ignore_index=True)\n",
    "X = postcode_ranking[TrainingColumns]\n",
    "y = postcode_ranking['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = remaining_postcode[TrainingColumns]\n",
    "remaining_postcode['Label'] = classifier.predict(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_postcode = remaining_postcode[remaining_postcode.Label == 1].reset_index().drop(['index'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining = df_remaining.rename(columns = {'cust_postcode':'postcode'})\n",
    "df_remaining_postcode  = pd.merge(df_remaining, remaining_postcode, on = 'postcode')\n",
    "final_postcode = pd.merge(df_remaining_postcode, df_postcode, on = \"postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = gmaps.figure(center=(-37.8102, 144.96), zoom_level=5)\n",
    "marker_layer = gmaps.marker_layer(final_postcode[['lat','lon']], info_box_content= ['postcode:' + str(i) + \" coverts \" + str(j) + \" top segment customers out of \" + str(k) for i,j,k in zip(final_postcode.postcode, final_postcode.custid_x, final_postcode.custid_y)])\n",
    "fig.add_layer(marker_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining_postcode.to_csv(\"C:/Users/bajaj/Desktop/opportunity_postcode.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_top_postcodes = df_postcode_hotspots[['cust_postcode','ratio','lat','lon']]\n",
    "company_opportunity_postcodes = final_postcode[['postcode','ratio','lat','lon']]\n",
    "company_top_postcodes['cmpnyid'] = analysis.company\n",
    "company_opportunity_postcodes['cmpnyid'] = analysis.company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_opportunity_postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This will create a dictionary of data types from BQ to Postgres\"\"\"\n",
    "def data_types(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    ul = [str(i) for i in df.dtypes]\n",
    "    datatypes = list(map(lambda x: \" INTEGER\" if 'int' in x else \" DECIMAL\" if 'float' in x else \" TEXT\" if 'object' in x else \" VARCHAR(255)\", ul))\n",
    "    return datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_pg(data, tablename, connection_string = None, schema = \"\"):\n",
    "    df = data.copy()\n",
    "    sio = StringIO()\n",
    "    sio.write(df.to_csv(index=None, header=None))\n",
    "    sio.seek(0)\n",
    "    file_object = sio\n",
    "    connection = psycopg2.connect(connection_string)\n",
    "    destination = \"{}.{}\".format(schema, tablename)\n",
    "    datatypes = data_types(df)\n",
    "    columns = \", \".join([(i + \" \" + j) for i,j in zip(df.columns, datatypes)])\n",
    "    schema_command = \"\"\"create schema IF NOT EXISTS {};\"\"\".format(schema)\n",
    "    cur = connection.cursor()\n",
    "    cur.execute(schema_command)\n",
    "    connection.commit()\n",
    "    command = \"\"\"create table IF NOT EXISTS {}({});\"\"\".format(destination,columns)\n",
    "    logging.info(command)\n",
    "    cur = connection.cursor()\n",
    "    cur.execute(command)\n",
    "    connection.commit()\n",
    "    with connection.cursor() as c:\n",
    "        columns = ', '.join([f'{col}' for col in df.columns])\n",
    "        sql = f'COPY {destination} ({columns}) FROM STDIN WITH CSV'\n",
    "        c.copy_expert(sql=sql, file=file_object)\n",
    "        connection.commit()\n",
    "        c.close()\n",
    "        del c\n",
    "    return logging.info(\"transfer successful for table {}\".format(destination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_to_pg(company_top_postcodes,tablename='company_top_postcodes', connection_string=connection_string, schema='feature')\n",
    "write_data_to_pg(company_opportunity_postcodes,tablename='company_opportunity_postcodes', connection_string=connection_string, schema='feature')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
